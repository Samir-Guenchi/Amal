\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{amsmath}

\geometry{margin=2.5cm}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    numbers=left,
    numbersep=5pt,
    tabsize=2
}
\lstset{style=mystyle}

\title{
    \textbf{Empathetic Support Model for Drug Addiction Recovery} \\
    \large Qwen2.5-7B with LoRA Fine-tuning
}
\author{Samir Guenchi}
\date{December 2024}

\begin{document}

\maketitle

\begin{abstract}
This report presents the development of an empathetic support model for the Amal drug addiction recovery platform. The model uses Qwen2.5-7B-Instruct as a base with Low-Rank Adaptation (LoRA) fine-tuning on empathetic dialogue data. The goal is to provide compassionate, non-judgmental responses to users seeking emotional support during their recovery journey.
\end{abstract}

\tableofcontents
\newpage

\section{Introduction}

While the RAG system handles factual queries, users seeking emotional support require a different approach. The support model aims to:

\begin{itemize}
    \item Provide empathetic, validating responses
    \item Avoid judgment or criticism
    \item Encourage professional help when appropriate
    \item Recognize crisis situations and refer to hotline
    \item Support multiple languages including Algerian Darija
\end{itemize}

\section{Model Selection}

\subsection{Why Qwen2.5-7B-Instruct?}

Qwen2.5-7B-Instruct was selected as the base model:

\begin{enumerate}
    \item \textbf{Multilingual}: Strong performance in Arabic and French
    \item \textbf{Size}: 7B parameters balances quality and resource requirements
    \item \textbf{Instruction-tuned}: Already trained for conversational tasks
    \item \textbf{Open Source}: Apache 2.0 license allows commercial use
    \item \textbf{Quality}: Competitive with larger models on benchmarks
\end{enumerate}

\begin{table}[h]
\centering
\begin{tabular}{@{}llll@{}}
\toprule
Model & Parameters & Arabic & License \\
\midrule
Qwen2.5-7B & 7B & Excellent & Apache 2.0 \\
Llama-3-8B & 8B & Limited & Llama License \\
Mistral-7B & 7B & Moderate & Apache 2.0 \\
AceGPT-7B & 7B & Good & Research only \\
\bottomrule
\end{tabular}
\caption{Base Model Comparison}
\end{table}

\subsection{Why LoRA?}

Low-Rank Adaptation (LoRA) enables efficient fine-tuning:

\begin{enumerate}
    \item \textbf{Parameter Efficiency}: Only 0.1\% of parameters trained
    \item \textbf{Memory Efficient}: Fits on consumer GPUs
    \item \textbf{Preserves Base Knowledge}: Minimal catastrophic forgetting
    \item \textbf{Portable}: Small adapter files (~50MB)
\end{enumerate}

\section{LoRA Architecture}

\subsection{Mathematical Foundation}

LoRA decomposes weight updates into low-rank matrices:

\begin{equation}
W' = W + \Delta W = W + BA
\end{equation}

Where:
\begin{itemize}
    \item $W \in \mathbb{R}^{d \times k}$ is the original weight matrix
    \item $B \in \mathbb{R}^{d \times r}$ and $A \in \mathbb{R}^{r \times k}$
    \item $r \ll \min(d, k)$ is the rank (typically 8-64)
\end{itemize}

\subsection{Configuration}

\begin{lstlisting}[language=Python, caption=LoRA Configuration]
peft_config = LoraConfig(
    r=16,                    # Rank
    lora_alpha=32,           # Scaling factor
    target_modules=[
        "q_proj",            # Query projection
        "v_proj",            # Value projection
    ],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
\end{lstlisting}

\section{Training Data}

\subsection{Data Sources}

The fine-tuning dataset includes:

\begin{itemize}
    \item Empathetic dialogue datasets (EmpatheticDialogues)
    \item Counseling conversation transcripts
    \item Synthetic conversations generated with GPT-4
    \item Translated and adapted for Arabic/Darija
\end{itemize}

\subsection{Data Format}

Training examples follow the instruction format:

\begin{lstlisting}[caption=Training Example Format]
<|im_start|>system
You are an empathetic counselor supporting someone 
in drug addiction recovery. Be compassionate and 
non-judgmental.
<|im_end|>
<|im_start|>user
I feel so tired from this addiction.
<|im_end|>
<|im_start|>assistant
I hear you, and I want you to know that feeling 
tired is completely understandable. Recovery is 
hard work, and it's okay to feel exhausted...
<|im_end|>
\end{lstlisting}

\section{Training Process}

\subsection{Hardware Requirements}

\begin{itemize}
    \item GPU: NVIDIA A100 40GB (or equivalent)
    \item Training time: ~4 hours for 3 epochs
    \item Inference: 8GB+ VRAM with 4-bit quantization
\end{itemize}

\subsection{Training Configuration}

\begin{lstlisting}[language=Python, caption=Training Arguments]
training_args = TrainingArguments(
    output_dir="./phase3-final",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    warmup_steps=100,
    logging_steps=10,
    save_strategy="epoch",
    fp16=True,
)
\end{lstlisting}

\section{Inference}

\subsection{Loading the Model}

\begin{lstlisting}[language=Python, caption=Model Loading]
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# Load base model with 4-bit quantization
base_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    load_in_4bit=True,
    device_map="auto"
)

# Load LoRA adapter
model = PeftModel.from_pretrained(
    base_model,
    "./working/phase3-final"
)
\end{lstlisting}

\subsection{Response Generation}

\begin{lstlisting}[language=Python, caption=Generation Code]
def generate_response(self, query: str, language: str):
    system_prompts = {
        "ar": "You are an empathetic counselor. Respond in Arabic.",
        "fr": "You are an empathetic counselor. Respond in French.",
        "dz": "You are an empathetic counselor. Respond in Darija."
    }
    
    messages = [
        {"role": "system", "content": system_prompts[language]},
        {"role": "user", "content": query}
    ]
    
    inputs = self.tokenizer.apply_chat_template(
        messages, return_tensors="pt"
    )
    
    outputs = self.model.generate(
        inputs,
        max_new_tokens=256,
        temperature=0.7,
        do_sample=True
    )
    
    return self.tokenizer.decode(outputs[0])
\end{lstlisting}

\section{Safety Considerations}

\subsection{Crisis Detection}

The model is trained to recognize crisis indicators:

\begin{itemize}
    \item Suicidal ideation
    \item Self-harm mentions
    \item Overdose risk
\end{itemize}

When detected, responses include the 3033 crisis hotline.

\subsection{Limitations}

\begin{itemize}
    \item Not a replacement for professional counseling
    \item May generate inappropriate responses in edge cases
    \item Requires human oversight for deployment
\end{itemize}

\section{Evaluation}

\subsection{Metrics}

\begin{table}[h]
\centering
\begin{tabular}{@{}lr@{}}
\toprule
Metric & Score \\
\midrule
Empathy (human eval) & 4.2/5 \\
Relevance & 4.0/5 \\
Safety & 4.5/5 \\
Language quality & 4.1/5 \\
\bottomrule
\end{tabular}
\caption{Human Evaluation Results}
\end{table}

\section{Future Work}

\begin{enumerate}
    \item Quantization for faster inference (GPTQ, AWQ)
    \item Streaming response generation
    \item More Algerian dialect training data
    \item Integration with conversation history
\end{enumerate}

\section{Conclusion}

The Qwen2.5-7B model with LoRA fine-tuning provides empathetic support responses for the Amal platform. While resource-intensive, the model demonstrates strong performance in multilingual empathetic dialogue, complementing the factual RAG system.

\section{References}

\begin{enumerate}
    \item Hu et al. (2021). LoRA: Low-Rank Adaptation of Large Language Models. arXiv.
    \item Qwen Team (2024). Qwen2.5 Technical Report.
    \item Rashkin et al. (2019). Towards Empathetic Open-domain Conversation Models. ACL.
\end{enumerate}

\end{document}
