# Support Model Requirements
# ==========================

# Core ML
torch>=2.0.0
transformers>=4.35.0

# LoRA/PEFT
peft>=0.6.0

# Quantization (optional, for reduced VRAM)
bitsandbytes>=0.41.0
accelerate>=0.24.0

# Installation
# ------------
# pip install -r requirements.txt

# Hardware Requirements
# ---------------------
# - GPU: 8GB+ VRAM (with 4-bit quantization)
# - RAM: 16GB+
# - Disk: 20GB for model download

# First Run
# ---------
# The base model (Qwen2.5-7B-Instruct) will be downloaded
# automatically on first use (~15GB download).

# Usage
# -----
# from support_backend import SupportBackend
# backend = SupportBackend(load_in_4bit=True)
# response = backend.generate_response("your query", language="ar")
